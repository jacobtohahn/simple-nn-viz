{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Neural Network Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks are complex and intricate systems, and understanding their inner workings can be a difficult learning curve. Thankfully, Python programming makes it easy to create neural networks and visualize their training process.\n",
    "\n",
    "This file is also available as a script: [simple_nn_decision.py](https://github.com/jacobtohahn/simple-nn-viz/blob/main/simple_nn_decision.py)\n",
    "\n",
    "Or as a Jupyter Notebook: [simple_nn_decision.ipynb](https://github.com/jacobtohahn/simple-nn-viz/blob/main/simple_nn_decision.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you get started, it's necessary to import all of the required modules for the project. Ensure all of these modules are installed in your Python environment and run the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import math, random, os, time\n",
    "from numpy import array, vstack, linspace\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from IPython.display import clear_output, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow import get_logger, autograph, device\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from absl import logging\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the necessary modules imported, you can now generate an example dataset.\n",
    "\n",
    "A dataset is an array of data points called *samples*. Each sample consists of a set of values, called *features*, and an output value. During training, the neural network is given each sample and asked to predict the output value. Each training iteration is called an *epoch*, and the *batch size* controls how many samples the model sees per epoch. After each epoch, the model uses an optimization function to adjust its weights to minimize the error between its output and the samples in the dataset.\n",
    "\n",
    "Features can be thought of as single measurement points. For example, if you were creating a dataset to classify types of birds, one feature may be \"beak length\" and another may be \"wingspan\". The output could then be the corresponding breed of bird, like \"Cardinal\". In this case, each sample in your dataset would be formatted as: `[beak length, wingspan, breed]`. Indices 0 and 1 (`[beak length, wingspan]`) are the features and index 2 (`[breed]`) is the output.\n",
    "\n",
    "While there are numerous datasets available to download online, you can also generate datasets with code. For this demonstration, you will generate a dataset with two features (x and y coordinates) and a binary output, either a 1 or a 0. This type of problem, where there is a single binary output, is known as *binary classification*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is code for generating two different types of datasets. The first generates a random binary classification problem, and the second generates a spiral pattern.\n",
    "\n",
    "You can set how many samples to generate by adjusting the variable `samples` and the noise of the spiral data by adjusting `noise`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, noise = 5000, 0.3\n",
    "\n",
    "# Visualize the decision boundary per epoch. Slow.\n",
    "show_progress = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, generate the dataset by calling either `classification_data(samples)` or `spiral_data(samples, noise)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_data(samples=2000):\n",
    "    X, y = make_classification(samples, 2, n_informative=2, n_redundant=0)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "def spiral_data(samples=2000, noise=0.3):\n",
    "    points = []\n",
    "    n = samples/2\n",
    "    def randUniform(a, b):\n",
    "        return random.random() * (b - a) + a\n",
    "    def genSpiral(deltaT, label):\n",
    "        i = 0\n",
    "        while i < n:\n",
    "            r = i / n * 5\n",
    "            t = 1.75 * i / n * 2 * math.pi + deltaT\n",
    "            x = r * math.sin(t) + randUniform(-1, 1) * noise\n",
    "            y = r * math.cos(t) + randUniform(-1, 1) * noise\n",
    "            points.append([x, y, label])\n",
    "            i = i + 1\n",
    "    genSpiral(0, 1)\n",
    "    genSpiral(math.pi, 0)\n",
    "    X = array(points)[:,:2]\n",
    "    y = array(points)[:,2]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y, X_train, X_test, y_train, y_test = spiral_data(samples) # <--- Call either classification_data(samples) or spiral_data(samples, noise)\n",
    "\n",
    "# Create a colormap for the output colors\n",
    "newcolors = vstack((linspace((242/255, 151/255, 31/255), (1, 1, 1)), linspace((1, 1, 1), (68/255, 145/255, 227/255))))\n",
    "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
    "\n",
    "# Visualize the dataset\n",
    "datafig, dataax = plt.subplots(figsize=(6,5))\n",
    "dataax.scatter(X[:,0], X[:,1], c=y, cmap=newcmp, marker='o', edgecolors=\"black\")\n",
    "dataax.set_title(\"Dataset:\")\n",
    "display(plt.gcf())\n",
    "clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've created a dataset, you need to create a neural network to train on the data.\n",
    "\n",
    "Firstly, surpress info messages from Tensorflow and absl. You can always change this later if you need more verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set all logging to error level\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "get_logger().setLevel('ERROR')\n",
    "autograph.set_verbosity(0)\n",
    "logging.set_verbosity(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, define a function to create the model. This is where you define the model's layers and nodes, as well as the loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating the model called by KerasClassifier()\n",
    "def create_model(learning_rate=0.01):\n",
    "    # Define the Keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add some layers and nodes\n",
    "    model.add(Dense(8, input_shape=(2,), activation=\"relu\"))\n",
    "    model.add(Dense(8, activation=\"relu\"))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Compile the Keras model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create an instance of the model wrapped in a KerasClassifer object. Additionally, a callback to print the training progress and visualize the model is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a callback to visualize progress\n",
    "class ShowDisplayBound(Callback):\n",
    "    # Runs at the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch%5 == 0:\n",
    "            # Get the model's prediction using the test data\n",
    "            predicted_y = pipeline.predict(X_test)\n",
    "            difference = differences(y_test, predicted_y)\n",
    "            accuracy = (samples/4 - difference) / (samples/4) * 100\n",
    "            if show_progress == False:\n",
    "                clear_output(wait=True)\n",
    "            # If show_progress is True, visualize the model's decision boundary for each epoch\n",
    "            # The decision boundary shows the model's prediction probability for a 100x100 grid over the domain of the dataset\n",
    "            if show_progress == True:\n",
    "                # Clear the previous epoch's plot\n",
    "                ax.clear()\n",
    "                # Plot the decision boundary of the model\n",
    "                DecisionBoundaryDisplay.from_estimator(pipeline, X_test, grid_resolution=100, cmap=newcmp, alpha=1, ax=ax)\n",
    "                # Plot the predicted data points\n",
    "                ax.scatter(X_test[:,0], X_test[:,1], c=predicted_y, cmap=newcmp, marker='o', edgecolors=\"black\")\n",
    "                ax.set_title(\"Predicted Output:\")\n",
    "                fig.suptitle(f\"Epoch {epoch}\\nAccuracy: {round(accuracy, 2)}%\")\n",
    "                # Show the plot\n",
    "                display(plt.gcf(), clear=True)\n",
    "            # Print progress bar and accuracy\n",
    "            display(f\"{tqdm.format_meter(epoch, epochnum, time.time() - t_start)} Accuracy: {round(accuracy, 2)}%\")\n",
    "\n",
    "        if epoch == epochnum - 1:\n",
    "            clear_output()\n",
    "            predicted_y = pipeline.predict(X_test)\n",
    "            difference = differences(y_test, predicted_y)\n",
    "            accuracy = (samples/4 - difference) / (samples/4) * 100\n",
    "            ax.clear()\n",
    "            DecisionBoundaryDisplay.from_estimator(pipeline, X_test, grid_resolution=100, cmap=newcmp, alpha=1, ax=ax)\n",
    "            ax.scatter(X_test[:,0], X_test[:,1], c=predicted_y, cmap=newcmp, marker='o', edgecolors=\"black\")\n",
    "            ax.set_title(\"Predicted Output:\")\n",
    "            fig.suptitle(f\"Epoch {epoch}\\nAccuracy: {round(accuracy, 2)}%\")\n",
    "            plt.gcf()\n",
    "            plt.pause(0.001)\n",
    "\n",
    "# Set the number of training epochs\n",
    "epochnum = 750\n",
    "\n",
    "# Create the KerasClassifier model. -1 batch size means all samples per epoch.\n",
    "clf = KerasClassifier(model=create_model, batch_size=-1, epochs=epochnum, learning_rate=0.02, verbose=0, callbacks=[ShowDisplayBound])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before passing the data to the model, it should be standardized. The StandardScalar normalizes each feature by removing the mean and scaling it to unit variance, but it is sensitive to outliers.\n",
    "\n",
    "To ensure that standarization is applied to all data that goes into the model and to keep training and test data separate, a Pipeline should be used. Data is inputted to the pipeline, and from there it is passed to each estimator. First, add the StandardScalar, then add the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of estimators\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('clf', clf))\n",
    "\n",
    "# Create a Pipeline with the estimators\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model and pipeline have been set up, you can train it on the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for calculating accuracy\n",
    "def differences(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"Lists of different length.\")\n",
    "    return sum(i != j for i, j in zip(a, b))\n",
    "\n",
    "# Set up the plots for visualization\n",
    "global fig, ax\n",
    "fig = plt.figure(figsize=(11,5))\n",
    "# One subplot for the prediction and one for the dataset\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.scatter(X[:,0], X[:,1], c=y, cmap=newcmp, marker='o', edgecolors=\"black\")\n",
    "ax2.set_title(\"Original dataset:\")\n",
    "\n",
    "# Fit the model on the training data. Each epoch, the callback ShowDisplayBound() is executed.\n",
    "with device(\"CPU\"):\n",
    "    t_start = time.time()\n",
    "    pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Model Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When creating and training a model, it can be useful to gather additional metrics about its performance. This is especially necessary for models with more than two input features, because a model with n input features produces an n-dimensional decision boundary. Beyond 3D, visualizing the decision boundary for all features becomes very difficult or impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a new dataset and model with 20 input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LearningCurveDisplay\n",
    "from tensorflow.keras.regularizers import L2\n",
    "\n",
    "samples = 2000\n",
    "\n",
    "def classification_data(samples=2000):\n",
    "    X, y = make_classification(samples, 20)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    return X, y, X_train, X_test, y_train, y_test\n",
    "\n",
    "X, y, X_train, X_test, y_train, y_test = classification_data(samples)\n",
    "\n",
    "def create_model(learning_rate=0.01):\n",
    "    # Define the Keras model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add some layers and nodes\n",
    "    model.add(Dense(16, input_shape=(20,), activation=\"relu\", kernel_regularizer=L2(0.025)))\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))\n",
    "    \n",
    "    # Compile the Keras model\n",
    "    opt = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "        \n",
    "    return model\n",
    "\n",
    "epochnum = 150\n",
    "# Create the KerasClassifier model. -1 batch size means all samples per epoch.\n",
    "clf = KerasClassifier(model=create_model, batch_size=-1, epochs=epochnum, learning_rate=0.01, verbose=0)\n",
    "\n",
    "# Create a list of estimators\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('clf', clf))\n",
    "\n",
    "# Create a Pipeline with the estimators\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model and plot a graph of its accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with device(\"CPU\"):\n",
    "    LearningCurveDisplay.from_estimator(pipeline, X_train, y_train, scoring=\"accuracy\", negate_score=False)\n",
    "    print(f\"Accuracy: {pipeline.fit(X_train, y_train).score(X_test, y_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
